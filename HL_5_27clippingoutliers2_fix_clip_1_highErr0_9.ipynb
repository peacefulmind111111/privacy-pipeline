{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "jaIuJgdQj9sS",
        "outputId": "676679cb-5c79-4819-d53c-b486dc66bb1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m176.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.7 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.3 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "868a6a888d764853be8c678138f66f8f",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m168.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m143.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m154.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m145.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m145.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m140.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m164.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.2.0\n",
            "\u001b[2K    Uninstalling triton-3.2.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.2.0\n",
            "\u001b[2K  Attempting uninstall: sympy\n",
            "\u001b[2K    Found existing installation: sympy 1.13.1\n",
            "\u001b[2K    Uninstalling sympy-1.13.1:\n",
            "\u001b[2K      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torch-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.21.0+cu124\n",
            "\u001b[2K    Uninstalling torchvision-0.21.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchaudio\n",
            "\u001b[2K    Found existing installation: torchaudio 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torchaudio-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.23.5)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.7.0+cu118)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus) (1.15.3)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=2.0->opacus) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n",
            "Downloading opacus-1.5.4-py3-none-any.whl (254 kB)\n",
            "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib, opacus\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [opacus]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.10.3 opacus-1.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --force-reinstall --no-cache-dir numpy==1.23.5\n",
        "# Install a specific version of torch that is known to have RMSNorm\n",
        "# Changed PyTorch version to a potentially more recent one with RMSNorm\n",
        "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --upgrade opacus matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkFLvQgBheQR",
        "outputId": "9edcc197-a4ec-46c8-c217-fe63d6abfe69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 15.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch=01] step=150 Loss=2.078 Acc=28.45% eps=0.86\n",
            "[Epoch=02] step=300 Loss=1.780 Acc=36.36% eps=1.17\n",
            "[Epoch=03] step=450 Loss=1.609 Acc=40.09% eps=1.43\n",
            "[Epoch=04] step=600 Loss=1.455 Acc=48.25% eps=1.65\n",
            "[Epoch=05] step=750 Loss=1.356 Acc=52.17% eps=1.85\n",
            "[Epoch=06] step=900 Loss=1.248 Acc=52.73% eps=2.03\n",
            "[Epoch=07] step=1050 Loss=1.171 Acc=57.29% eps=2.20\n",
            "[Epoch=08] step=1200 Loss=1.099 Acc=61.45% eps=2.36\n",
            "[Epoch=09] step=1350 Loss=1.023 Acc=60.52% eps=2.51\n",
            "[Epoch=10] step=1500 Loss=0.972 Acc=66.14% eps=2.65\n",
            "[Epoch=11] step=1650 Loss=0.951 Acc=65.97% eps=2.79\n",
            "[Epoch=12] step=1800 Loss=0.882 Acc=68.81% eps=2.92\n",
            "[Epoch=13] step=1950 Loss=0.839 Acc=68.02% eps=3.05\n",
            "[Epoch=14] step=2100 Loss=0.814 Acc=69.59% eps=3.18\n",
            "[Epoch=15] step=2250 Loss=0.771 Acc=70.38% eps=3.30\n",
            "[Epoch=16] step=2400 Loss=0.759 Acc=66.15% eps=3.42\n",
            "[Epoch=17] step=2550 Loss=0.751 Acc=69.31% eps=3.53\n",
            "[Epoch=18] step=2700 Loss=0.721 Acc=73.79% eps=3.64\n",
            "[Epoch=19] step=2850 Loss=0.712 Acc=73.13% eps=3.75\n",
            "[Epoch=20] step=3000 Loss=0.708 Acc=75.72% eps=3.86\n",
            "[Epoch=21] step=3150 Loss=0.705 Acc=76.29% eps=3.96\n",
            "[Epoch=22] step=3300 Loss=0.684 Acc=71.65% eps=4.07\n",
            "[Epoch=23] step=3450 Loss=0.689 Acc=76.14% eps=4.17\n",
            "[Epoch=24] step=3600 Loss=0.662 Acc=73.34% eps=4.27\n",
            "[Epoch=25] step=3750 Loss=0.645 Acc=76.59% eps=4.36\n",
            "[Epoch=26] step=3900 Loss=0.641 Acc=76.06% eps=4.46\n",
            "[Epoch=27] step=4050 Loss=0.642 Acc=72.94% eps=4.56\n",
            "[Epoch=28] step=4200 Loss=0.654 Acc=76.50% eps=4.65\n",
            "[Epoch=29] step=4350 Loss=0.623 Acc=78.86% eps=4.74\n",
            "[Epoch=30] step=4500 Loss=0.613 Acc=77.83% eps=4.83\n",
            "[Epoch=31] step=4650 Loss=0.474 Acc=81.85% eps=4.92\n",
            "[Epoch=32] step=4800 Loss=0.442 Acc=82.53% eps=5.01\n",
            "[Epoch=33] step=4950 Loss=0.431 Acc=82.79% eps=5.10\n",
            "[Epoch=34] step=5100 Loss=0.421 Acc=82.97% eps=5.18\n",
            "[Epoch=35] step=5250 Loss=0.416 Acc=82.75% eps=5.27\n",
            "[Epoch=36] step=5400 Loss=0.409 Acc=83.27% eps=5.35\n",
            "[Epoch=37] step=5550 Loss=0.403 Acc=83.31% eps=5.44\n",
            "[Epoch=38] step=5700 Loss=0.400 Acc=83.53% eps=5.52\n",
            "[Epoch=39] step=5850 Loss=0.393 Acc=83.89% eps=5.60\n",
            "[Epoch=40] step=6000 Loss=0.388 Acc=83.57% eps=5.68\n",
            "[Epoch=41] step=6150 Loss=0.385 Acc=83.51% eps=5.76\n",
            "[Epoch=42] step=6300 Loss=0.381 Acc=83.99% eps=5.84\n",
            "[Epoch=43] step=6450 Loss=0.377 Acc=83.79% eps=5.92\n",
            "[Epoch=44] step=6600 Loss=0.374 Acc=84.04% eps=6.00\n",
            "[Epoch=45] step=6750 Loss=0.369 Acc=83.93% eps=6.07\n",
            "[Epoch=46] step=6900 Loss=0.360 Acc=84.36% eps=6.15\n",
            "[Epoch=47] step=7050 Loss=0.357 Acc=84.40% eps=6.23\n",
            "[Epoch=48] step=7200 Loss=0.355 Acc=84.44% eps=6.30\n",
            "[Epoch=49] step=7350 Loss=0.354 Acc=84.43% eps=6.38\n",
            "[Epoch=50] step=7500 Loss=0.355 Acc=84.40% eps=6.45\n",
            "\n",
            "Done. (Batch-sum clipping, outliers=0.5 after 60%). Final Acc=84.40% eps=6.45\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "\n",
        "# Opacus\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.grad_sample import GradSampleModule\n",
        "from opacus.accountants import RDPAccountant\n",
        "\n",
        "################################\n",
        "# 0. Hyperparameters\n",
        "################################\n",
        "seed = 0\n",
        "batch_size      = 1000\n",
        "lr              = 0.1\n",
        "outer_momentum  = 0.9\n",
        "inner_momentum  = 0.08\n",
        "noise_mult      = 1.5\n",
        "delta           = 1e-5\n",
        "num_epochs      = 50\n",
        "self_aug_factor = 3\n",
        "M               = 10000\n",
        "\n",
        "DEFAULT_CLIP    = 1     # normal clip\n",
        "OUTLIER_CLIP    = 0.5     # outlier clip\n",
        "HIGH_ERR_THRESHOLD  = 0.9 # if error ≥95% => outlier\n",
        "DROP_AFTER_FRAC     = 0.6  # only apply outlier clip after 60% training\n",
        "\n",
        "SCHEDULE_MILESTONES = [30, 45]\n",
        "SCHEDULE_GAMMA       = 0.1\n",
        "\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "################################\n",
        "# 1. Dataset\n",
        "################################\n",
        "mean, std = [0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02,0.2)),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True,  download=True, transform=train_tf)\n",
        "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "n = len(train_ds)\n",
        "\n",
        "class IndexedSubset(Subset):\n",
        "    \"\"\"Wrap CIFAR-10 so each sample returns (x, y, idx).\"\"\"\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = super().__getitem__(idx)\n",
        "        return x, y, idx\n",
        "\n",
        "train_sub = IndexedSubset(train_ds, range(n))\n",
        "train_full = ConcatDataset([train_sub]*self_aug_factor)\n",
        "train_loader = DataLoader(train_full, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(test_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "################################\n",
        "# 2. ResNet20\n",
        "################################\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
        "        self.gn1   = nn.GroupNorm(8, planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
        "        self.gn2   = nn.GroupNorm(8, planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
        "                nn.GroupNorm(8, planes)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.gn1(self.conv1(x)))\n",
        "        out = self.gn2(self.conv2(out))\n",
        "        # avoid in-place modification on out\n",
        "        out = out.clone() + self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1, bias=False)\n",
        "        self.gn1   = nn.GroupNorm(8,16)\n",
        "        self.layer1 = self._make_layer(16, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 3, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 3, stride=2)\n",
        "        self.avgpool= nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc     = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers=[]\n",
        "        for s in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, s))\n",
        "            self.in_planes= planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.gn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out,1)\n",
        "        return self.fc(out)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total   = 0\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y = X.to(device), y.to(device)\n",
        "            preds = model(X).argmax(dim=1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    if was_training:\n",
        "        model.train()\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "################################\n",
        "# 3. Build DP ResNet\n",
        "################################\n",
        "def build_model():\n",
        "    net = ResNet20().to(device)\n",
        "    errs= ModuleValidator.validate(net, strict=False)\n",
        "    if errs:\n",
        "        net = ModuleValidator.fix(net).to(device)\n",
        "    return GradSampleModule(net)\n",
        "\n",
        "################################\n",
        "# 4. Sliding Window Momentum\n",
        "################################\n",
        "class LRUOrderedDict(OrderedDict):\n",
        "    def __init__(self, *args, maxsize=10000, **kwargs):\n",
        "        self.maxsize = maxsize\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def __getitem__(self,key):\n",
        "        val = super().__getitem__(key)\n",
        "        self.move_to_end(key)\n",
        "        return val\n",
        "\n",
        "    def __setitem__(self,key,val):\n",
        "        if key in self:\n",
        "            self.move_to_end(key)\n",
        "        super().__setitem__(key,val)\n",
        "        if len(self) > self.maxsize:\n",
        "            self.popitem(last=False)\n",
        "\n",
        "momentum_dict = LRUOrderedDict(maxsize=M)\n",
        "\n",
        "#############################################\n",
        "# 5. Aggregated Grad Computation (inner momentum)\n",
        "#############################################\n",
        "def compute_inner_momentum_grads_idxed(dp_model, X, y, idxs):\n",
        "    \"\"\"\n",
        "    - forward/backward => gather per-sample grads\n",
        "    - average duplicates (from self_aug_factor)\n",
        "    - apply \"inner momentum\"\n",
        "    - return one vector per unique sample_id\n",
        "    \"\"\"\n",
        "    dp_model.zero_grad()\n",
        "    out = dp_model(X)\n",
        "    loss= F.cross_entropy(out, y)\n",
        "    loss.backward()\n",
        "\n",
        "    bs = X.size(0)\n",
        "    param_vecs= [None]*bs\n",
        "\n",
        "    for p in dp_model.parameters():\n",
        "        gs = getattr(p,\"grad_sample\",None)\n",
        "        if gs is None:\n",
        "            continue\n",
        "        gs_flat= gs.view(bs,-1).detach()\n",
        "        for i in range(bs):\n",
        "            if param_vecs[i] is None:\n",
        "                param_vecs[i]= gs_flat[i]\n",
        "            else:\n",
        "                param_vecs[i]= torch.cat([param_vecs[i], gs_flat[i]], dim=0)\n",
        "        p.grad_sample= None\n",
        "\n",
        "    # group by sample_id => average => momentum\n",
        "    sample_to_vecs = {}\n",
        "    for i in range(bs):\n",
        "        sample_id= int(idxs[i].item())\n",
        "        if sample_id not in sample_to_vecs:\n",
        "            sample_to_vecs[sample_id] = []\n",
        "        sample_to_vecs[sample_id].append(param_vecs[i].to(device))\n",
        "\n",
        "    batch_v=[]\n",
        "    for sample_id, grads_list in sample_to_vecs.items():\n",
        "        g_i = torch.stack(grads_list, dim=0).mean(dim=0)\n",
        "\n",
        "        if sample_id in momentum_dict:\n",
        "            old_v_half = momentum_dict[sample_id]\n",
        "            old_v = old_v_half.to(device)\n",
        "        else:\n",
        "            old_v = torch.zeros_like(g_i)\n",
        "\n",
        "        new_v = inner_momentum*old_v + (1.0 - inner_momentum)*g_i\n",
        "        momentum_dict[sample_id] = new_v.half().cpu()\n",
        "        batch_v.append(new_v)\n",
        "\n",
        "    return batch_v, len(sample_to_vecs)\n",
        "\n",
        "################################\n",
        "# 6. Single-Sum Clipping & Noise\n",
        "################################\n",
        "def clip_and_add_noise_outliers_batch(batch_v, clip_vals):\n",
        "    \"\"\"\n",
        "    1. Sum all vectors in batch_v into 'grad_sum'.\n",
        "    2. If norm(grad_sum) > sum(clip_vals), rescale it down\n",
        "       so that norm(grad_sum)= sum(clip_vals).\n",
        "    3. Add noise with std = noise_mult * max(clip_vals).\n",
        "    4. Return grad_sum / len(batch_v).\n",
        "    \"\"\"\n",
        "    # batch_v: a list of per-sample momentum vectors\n",
        "    # clip_vals: matching list of clip thresholds, either 5.0 or 0.5\n",
        "    device = batch_v[0].device\n",
        "    count  = len(batch_v)\n",
        "\n",
        "    # sum them up\n",
        "    grad_sum = torch.stack(batch_v, dim=0).sum(dim=0)\n",
        "    norm_ = grad_sum.norm(2)\n",
        "\n",
        "    # threshold = sum of individual clip_i across the batch\n",
        "    # so if each sample had c_i, we can hold the sum to sum(clip_vals).\n",
        "    threshold = sum(clip_vals)\n",
        "    if norm_ > threshold:\n",
        "        scale = threshold / (norm_ + 1e-6)\n",
        "        grad_sum = grad_sum * scale\n",
        "\n",
        "    # add noise using the largest clip in this batch\n",
        "    cmax = max(clip_vals)\n",
        "    noise = torch.randn_like(grad_sum) * (noise_mult * cmax)\n",
        "    grad_sum += noise\n",
        "\n",
        "    # average over the number of distinct samples in the batch\n",
        "    return grad_sum / count\n",
        "\n",
        "def outer_step_outliers_batch(dp_model, optimizer, batch_v, clip_vals):\n",
        "    final_grad = clip_and_add_noise_outliers_batch(batch_v, clip_vals)\n",
        "    idx_start  = 0\n",
        "    for p in dp_model.parameters():\n",
        "        numel = p.numel()\n",
        "        chunk = final_grad[idx_start: idx_start + numel]\n",
        "        p.grad= chunk.view_as(p)\n",
        "        idx_start += numel\n",
        "    optimizer.step()\n",
        "\n",
        "################################\n",
        "# 7. Train => HighErr => clip=0.5 (batch-sum style)\n",
        "################################\n",
        "def train_clip_outliers_batchsum():\n",
        "    \"\"\"\n",
        "    - For the first 60% of steps => everyone gets clip=5.0\n",
        "    - After 60%, if a sample's running error >=95% => clip=0.5\n",
        "      else => clip=5.0\n",
        "    - Then do single-sum clipping & noise (like budget code)\n",
        "    \"\"\"\n",
        "    global momentum_dict\n",
        "    momentum_dict.clear()\n",
        "\n",
        "    dp_net= build_model().to(device)\n",
        "    optimizer= optim.SGD(dp_net.parameters(), lr=lr, momentum=outer_momentum, weight_decay=5e-4)\n",
        "    scheduler= optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer, milestones=SCHEDULE_MILESTONES, gamma=SCHEDULE_GAMMA)\n",
        "    accountant= RDPAccountant()\n",
        "\n",
        "    # track correctness stats\n",
        "    sample_correct_count= np.zeros(n, dtype=int)\n",
        "    sample_total_count  = np.zeros(n, dtype=int)\n",
        "\n",
        "    total_steps= num_epochs * len(train_loader)\n",
        "    step_count= 0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        dp_net.train()\n",
        "        losses=[]\n",
        "\n",
        "        for X,y,idxs in train_loader:\n",
        "            step_count+= 1\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # measure correctness quickly\n",
        "            with torch.no_grad():\n",
        "                out= dp_net(X)\n",
        "                preds= out.argmax(dim=1)\n",
        "                correct_vec= (preds == y).float()\n",
        "\n",
        "            # update per-sample correctness stats\n",
        "            for i, sidx in enumerate(idxs):\n",
        "                sidx_i= int(sidx.item())\n",
        "                sample_total_count[sidx_i]+=1\n",
        "                sample_correct_count[sidx_i]+= int(correct_vec[i].item())\n",
        "\n",
        "            # gather momentum vectors\n",
        "            batch_v, unique_count= compute_inner_momentum_grads_idxed(dp_net, X, y, idxs)\n",
        "\n",
        "            # quick measure loss\n",
        "            with torch.no_grad():\n",
        "                loss= F.cross_entropy(out,y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # build clip array\n",
        "            clip_vals=[]\n",
        "            if step_count <= DROP_AFTER_FRAC * total_steps:\n",
        "                # 0 to 60% => clip=5.0 for all\n",
        "                clip_vals= [DEFAULT_CLIP]* len(batch_v)\n",
        "            else:\n",
        "                # after 60%, check if error>=95% => clip=0.5\n",
        "                # else => 5.0\n",
        "                for i, sidx in enumerate(idxs):\n",
        "                    sidx_i= int(sidx.item())\n",
        "                    total_ = sample_total_count[sidx_i]\n",
        "                    corr_  = sample_correct_count[sidx_i]\n",
        "                    if total_>0:\n",
        "                        err_rate= 1.0 - (corr_/ total_)\n",
        "                        if err_rate <= HIGH_ERR_THRESHOLD:\n",
        "                            clip_vals.append(OUTLIER_CLIP)  # 0.5\n",
        "                        else:\n",
        "                            clip_vals.append(DEFAULT_CLIP)   # 5.0\n",
        "                    else:\n",
        "                        # if no stats yet, default clip\n",
        "                        clip_vals.append(DEFAULT_CLIP)\n",
        "\n",
        "            # single-sum clipping + noise\n",
        "            outer_step_outliers_batch(dp_net, optimizer, batch_v, clip_vals)\n",
        "\n",
        "            sample_rate= unique_count / 50000.0  # for CIFAR-10 base size=50k\n",
        "            accountant.step(noise_multiplier=noise_mult, sample_rate=sample_rate)\n",
        "\n",
        "        scheduler.step()\n",
        "        acc= evaluate(dp_net, test_loader)\n",
        "        eps= accountant.get_epsilon(delta)\n",
        "        print(f\"[Epoch={epoch:02d}] step={step_count} Loss={np.mean(losses):.3f} \"\n",
        "              f\"Acc={acc:.2f}% eps={eps:.2f}\")\n",
        "\n",
        "    final_acc= evaluate(dp_net, test_loader)\n",
        "    final_eps= accountant.get_epsilon(delta)\n",
        "    print(f\"\\nDone. (Batch-sum clipping, outliers=0.5 after 60%). \"\n",
        "          f\"Final Acc={final_acc:.2f}% eps={final_eps:.2f}\")\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    train_clip_outliers_batchsum()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}