{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L0fZpSmVuod8",
        "outputId": "937b657e-d0b2-4589-e51b-6c2cbc3f6ac9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4d620241e0834bd8966c9bd8bdb54df8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.2.0\n",
            "\u001b[2K    Uninstalling triton-3.2.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.2.0\n",
            "\u001b[2K  Attempting uninstall: sympy\n",
            "\u001b[2K    Found existing installation: sympy 1.13.1\n",
            "\u001b[2K    Uninstalling sympy-1.13.1:\n",
            "\u001b[2K      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torch-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.21.0+cu124\n",
            "\u001b[2K    Uninstalling torchvision-0.21.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchaudio\n",
            "\u001b[2K    Found existing installation: torchaudio 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torchaudio-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton) (75.2.0)\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.3.1)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (2.7.1+cu118)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->opacus) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=2.0->opacus) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n",
            "Downloading opacus-1.5.4-py3-none-any.whl (254 kB)\n",
            "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, matplotlib, opacus\n",
            "\u001b[2K  Attempting uninstall: scipy\n",
            "\u001b[2K    Found existing installation: scipy 1.15.3\n",
            "\u001b[2K    Uninstalling scipy-1.15.3:\n",
            "\u001b[2K      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [opacus]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.14.6 requires scipy<1.16.0,>=1.8.0, but you have scipy 1.16.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.10.3 opacus-1.5.4 scipy-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "b1204336d8e146048ce079b8000be992"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "# Remove the specific version constraint on numpy\n",
        "!pip install --upgrade --no-cache-dir numpy\n",
        "# Install a specific version of torch that is known to have RMSNorm\n",
        "# Changed PyTorch version to a potentially more recent one with RMSNorm\n",
        "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install Triton from the same index URL to ensure compatibility\n",
        "!pip install --upgrade triton --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "!pip install --upgrade opacus matplotlib scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "# Filename: dp_virtual_projection_population_only.py\n",
        "# (2025‑07‑03 patched version)\n",
        "##############################################################\n",
        "import os, psutil, random, math, time, statistics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "\n",
        "# Opacus\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.grad_sample import GradSampleModule\n",
        "\n",
        "###############################################################################\n",
        "# 0. Hyperparameters\n",
        "###############################################################################\n",
        "seed                 = 0\n",
        "random.seed(seed);  torch.manual_seed(seed);  np.random.seed(seed)\n",
        "\n",
        "batch_size           = 1000\n",
        "lr                   = 0.1\n",
        "outer_momentum       = 0.9\n",
        "inner_momentum       = 0.0\n",
        "noise_mult           = 0.0          # no DP noise in this script\n",
        "delta                = 1e-5\n",
        "num_epochs           = 10            # ← raise above warm_start_epochs for real runs\n",
        "M                    = 0             # momentum dictionary capacity\n",
        "self_aug_factor      = 1\n",
        "\n",
        "# --- Projection experiment knobs --------------------------------------------\n",
        "warm_start_epochs     = 2           # plain‑SGD epochs before any projection\n",
        "proj_lr_multiplier    = 3.0         # bump LR when projection begins\n",
        "scale_pprime_to_pL2   = True       # re‑norm p′ so ‖p′‖₂ = ‖p‖₂\n",
        "trust_mix_alpha       = 1           # 0→raw p ; 1→pure p′\n",
        "rebuild_basis_every   = 3           # epochs; 0→never rebuild\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -- Virtual‑sample parameters --\n",
        "subsets_per_class      = 25\n",
        "subset_size            = 200\n",
        "virtual_augment_factor = 10\n",
        "\n",
        "# -- “No‑clipping” setup (huge C so clipping never triggers)\n",
        "c_start = 1e9;  c_end = 1e9\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "###############################################################################\n",
        "# 0.1 Memory helper\n",
        "###############################################################################\n",
        "def print_memory_usage(msg=\"\"):\n",
        "    try:\n",
        "        mem_mb = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
        "        print(f\"[MEM] {msg} → {mem_mb:,.1f} MB\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "###############################################################################\n",
        "# 1. Dataset\n",
        "###############################################################################\n",
        "mean, std = [0.4914,0.4822,0.4466], [0.2470,0.2435,0.2616]  # tiny typo fix\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02,0.2)),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"./data\", train=True , download=True, transform=train_tf)\n",
        "test_ds  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "class IndexedSubset(Subset):\n",
        "    \"\"\"Return (x, y, idx) for momentum tracking.\"\"\"\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = super().__getitem__(idx)\n",
        "        return x, y, idx\n",
        "\n",
        "n = len(train_ds)\n",
        "train_sub  = IndexedSubset(train_ds, range(n))\n",
        "train_full = ConcatDataset([train_sub]*self_aug_factor)\n",
        "\n",
        "train_loader = DataLoader(train_full,batch_size=batch_size,shuffle=True,drop_last=True)\n",
        "test_loader  = DataLoader(test_ds ,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "###############################################################################\n",
        "# 2. ResNet‑20 (GroupNorm)\n",
        "###############################################################################\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self,in_planes,planes,stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes,planes,3,stride,1,bias=False)\n",
        "        self.gn1   = nn.GroupNorm(8,planes)\n",
        "        self.conv2 = nn.Conv2d(planes,planes,3,1,1,bias=False)\n",
        "        self.gn2   = nn.GroupNorm(8,planes)\n",
        "        self.short = nn.Sequential()\n",
        "        if stride!=1 or in_planes!=planes:\n",
        "            self.short = nn.Sequential(\n",
        "                nn.Conv2d(in_planes,planes,1,stride,bias=False),\n",
        "                nn.GroupNorm(8,planes)\n",
        "            )\n",
        "    def forward(self,x):\n",
        "        out = F.relu(self.gn1(self.conv1(x)))\n",
        "        out = self.gn2(self.conv2(out))\n",
        "        out = out + self.short(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self,num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3,16,3,1,1,bias=False)\n",
        "        self.gn1   = nn.GroupNorm(8,16)\n",
        "        self.layer1 = self._make_layer(16,3,1)\n",
        "        self.layer2 = self._make_layer(32,3,2)\n",
        "        self.layer3 = self._make_layer(64,3,2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc      = nn.Linear(64,num_classes)\n",
        "    def _make_layer(self,planes,blocks,stride):\n",
        "        strides=[stride]+[1]*(blocks-1); layers=[]\n",
        "        for s in strides:\n",
        "            layers.append(BasicBlock(self.in_planes,planes,s))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.gn1(self.conv1(x)))\n",
        "        x = self.layer1(x); x=self.layer2(x); x=self.layer3(x)\n",
        "        x = self.avgpool(x); x=torch.flatten(x,1)\n",
        "        return self.fc(x)\n",
        "\n",
        "def evaluate(model,loader):\n",
        "    was_training = model.training\n",
        "    model.eval(); correct=total=0\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y = X.to(device), y.to(device)\n",
        "            preds = model(X).argmax(1)\n",
        "            correct += (preds==y).sum().item(); total += y.size(0)\n",
        "    if was_training: model.train()\n",
        "    return 100.*correct/total\n",
        "\n",
        "###############################################################################\n",
        "# 3. Build DP‑ready network\n",
        "###############################################################################\n",
        "def build_model():\n",
        "    net = ResNet20().to(device)\n",
        "    if ModuleValidator.validate(net,strict=False):\n",
        "        net = ModuleValidator.fix(net).to(device)\n",
        "    return GradSampleModule(net)\n",
        "\n",
        "###############################################################################\n",
        "# 4. LRU‑momentum dict\n",
        "###############################################################################\n",
        "class LRUOrderedDict(OrderedDict):\n",
        "    def __init__(self,*a,maxsize=10_000,**kw): self.maxsize=maxsize; super().__init__(*a,**kw)\n",
        "    def __getitem__(self,key):\n",
        "        val=super().__getitem__(key); self.move_to_end(key); return val\n",
        "    def __setitem__(self,key,val):\n",
        "        if key in self: self.move_to_end(key)\n",
        "        super().__setitem__(key,val)\n",
        "        if len(self)>self.maxsize: self.popitem(last=False)\n",
        "\n",
        "momentum_dict = LRUOrderedDict(maxsize=M)\n",
        "\n",
        "###############################################################################\n",
        "# 5. Per‑sample momentum vector\n",
        "###############################################################################\n",
        "def compute_inner_momentum_grads_idxed(dp_model,X,y,idxs):\n",
        "    dp_model.zero_grad()\n",
        "    loss = F.cross_entropy(dp_model(X),y); loss.backward()\n",
        "    bs = X.size(0); param_vecs=[None]*bs\n",
        "    for p in dp_model.parameters():\n",
        "        gs = getattr(p,\"grad_sample\",None)\n",
        "        if gs is None: continue\n",
        "        gs_flat = gs.view(bs,-1).detach()\n",
        "        for i in range(bs):\n",
        "            param_vecs[i] = gs_flat[i] if param_vecs[i] is None \\\n",
        "                            else torch.cat([param_vecs[i],gs_flat[i]],0)\n",
        "        p.grad_sample=None\n",
        "    sample_to_vecs={}\n",
        "    for i in range(bs):\n",
        "        sid = int(idxs[i]); sample_to_vecs.setdefault(sid,[]).append(param_vecs[i].to(device))\n",
        "    batch_v=[]\n",
        "    for sid,vecs in sample_to_vecs.items():\n",
        "        g_i = torch.stack(vecs,0).mean(0)\n",
        "        old_v = momentum_dict[sid].to(device) if sid in momentum_dict \\\n",
        "                 else torch.zeros_like(g_i)\n",
        "        new_v = inner_momentum*old_v + (1-inner_momentum)*g_i\n",
        "        momentum_dict[sid] = new_v.half().cpu();  batch_v.append(new_v)\n",
        "    return batch_v, len(sample_to_vecs)\n",
        "\n",
        "###############################################################################\n",
        "# 6. Outer step (no clipping/no noise)\n",
        "###############################################################################\n",
        "def outer_step_noDP(dp_model,optimizer,batch_v):\n",
        "    grad_sum = torch.stack(batch_v,0).sum(0)\n",
        "    final_grad = grad_sum/len(batch_v)\n",
        "    idx=0\n",
        "    for p in dp_model.parameters():\n",
        "        numel=p.numel(); p.grad=final_grad[idx:idx+numel].view_as(p); idx+=numel\n",
        "    optimizer.step()\n",
        "\n",
        "###############################################################################\n",
        "# 7. Virtual subsets helpers\n",
        "###############################################################################\n",
        "def build_virtual_subsets(dataset,subsets_per_class,subset_size,augment_factor):\n",
        "    cls_idx=[[] for _ in range(10)]\n",
        "    for i in range(len(dataset)): _,c=dataset[i]; cls_idx[c].append(i)\n",
        "    v_subsets=[]\n",
        "    for c in range(10):\n",
        "        random.shuffle(cls_idx[c])\n",
        "        for k in range(subsets_per_class):\n",
        "            subset=cls_idx[c][k*subset_size:(k+1)*subset_size]\n",
        "            for _ in range(augment_factor): v_subsets.append((subset,c))\n",
        "    return v_subsets\n",
        "\n",
        "def compute_fullmodel_grad_for_subset(model,subset_idxs,dataset):\n",
        "    model.zero_grad()\n",
        "    X=torch.stack([dataset[i][0] for i in subset_idxs]).to(device)\n",
        "    y=torch.tensor([dataset[i][1] for i in subset_idxs],device=device)\n",
        "    loss = F.cross_entropy(model(X),y); loss.backward()\n",
        "    parts=[p.grad.view(-1).detach() for p in model.parameters() if p.grad is not None]\n",
        "    return torch.cat(parts).to(device)/len(subset_idxs)\n",
        "\n",
        "###############################################################################\n",
        "# 8. Gram–Schmidt\n",
        "###############################################################################\n",
        "def gram_schmidt(vecs,eps=1e-10):\n",
        "    basis=[]\n",
        "    for v in vecs:\n",
        "        w=v.clone().float().to(device)\n",
        "        for b in basis: w -= torch.dot(w,b)*b\n",
        "        n=w.norm()\n",
        "        if n>eps: basis.append(w/n)\n",
        "    return basis\n",
        "\n",
        "def build_e_basis_for(model):\n",
        "    \"\"\"Compute Gram–Schmidt‑orthonormalised gradients of all virtual subsets\n",
        "       using the *current* model parameters.\"\"\"\n",
        "    v_subsets = build_virtual_subsets(\n",
        "        train_ds,\n",
        "        subsets_per_class,\n",
        "        subset_size,\n",
        "        virtual_augment_factor,\n",
        "    )\n",
        "    e_list = [\n",
        "        compute_fullmodel_grad_for_subset(model, idxs, train_ds)\n",
        "        for idxs, _ in v_subsets\n",
        "    ]\n",
        "    return gram_schmidt(e_list)\n",
        "\n",
        "###############################################################################\n",
        "# 9. Unwrapped population grad\n",
        "###############################################################################\n",
        "def compute_population_grad_unwrapped(dp_model,X,y):\n",
        "    net = dp_model._module\n",
        "    net.zero_grad()\n",
        "    F.cross_entropy(net(X),y).backward()\n",
        "    parts=[p.grad.view(-1).detach() for p in net.parameters() if p.grad is not None]\n",
        "    return torch.cat(parts).to(device) / X.size(0)\n",
        "\n",
        "###############################################################################\n",
        "# 10-A. Training loop\n",
        "###############################################################################\n",
        "def run_training(dp_net,\n",
        "                 optimizer,\n",
        "                 want_projection,\n",
        "                 e_basis,\n",
        "                 project,\n",
        "                 train_loader,\n",
        "                 test_loader,\n",
        "                 diag_every:int=50):\n",
        "\n",
        "    epoch_losses, epoch_accs = [], []\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[], gamma=0.1)\n",
        "    momentum_dict.clear()\n",
        "\n",
        "    # <<< NEW -----------------------------------------------------------------\n",
        "    # Local trackers to diagnose the projection run as well\n",
        "    local_diff_L2, local_cos = [], []\n",
        "    # -------------------------------------------------------------------------\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        dp_net.train()\n",
        "        losses, recent_losses = [], []\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # ---- experiment-level toggles -------------------------------------\n",
        "        use_proj_this_epoch = want_projection and (epoch > warm_start_epochs)\n",
        "\n",
        "        # (new) after warm-start, rebuild ONCE so the basis matches current weights\n",
        "        if epoch == warm_start_epochs and want_projection:\n",
        "            e_basis[:] = build_e_basis_for(dp_net._module)\n",
        "\n",
        "        # optional periodic refresh\n",
        "        if rebuild_basis_every and (epoch % rebuild_basis_every == 0):\n",
        "            e_basis[:] = build_e_basis_for(dp_net._module)\n",
        "\n",
        "        if epoch == warm_start_epochs + 1 and proj_lr_multiplier != 1.0 and want_projection:\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] *= proj_lr_multiplier\n",
        "        if rebuild_basis_every and (epoch % rebuild_basis_every == 0):\n",
        "            e_basis[:] = gram_schmidt(e_basis)  # quick refresh\n",
        "        # -------------------------------------------------------------------\n",
        "\n",
        "        total_batches = len(train_loader)\n",
        "        for batch_idx, (X, y, idxs) in enumerate(train_loader, 1):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # ---- forward / backward ---------------------------------------\n",
        "            batch_v, _ = compute_inner_momentum_grads_idxed(dp_net, X, y, idxs)\n",
        "            p          = compute_population_grad_unwrapped(dp_net, X, y)\n",
        "            p_prime    = project(p, e_basis)\n",
        "\n",
        "            # <<< NEW: dimensionality guard ---------------------------------\n",
        "            assert p_prime.shape == p.shape, \\\n",
        "                f\"[Bug] p′ dim {p_prime.shape} ≠ p dim {p.shape}\"\n",
        "            # ----------------------------------------------------------------\n",
        "\n",
        "            if scale_pprime_to_pL2 and p_prime.norm() > 0:\n",
        "                p_prime = p_prime * (p.norm() / p_prime.norm())\n",
        "\n",
        "            g_update  = trust_mix_alpha * p_prime + (1 - trust_mix_alpha) * p\n",
        "            # ----------------------------------------------------------------\n",
        "\n",
        "            # choose update direction ---------------------------------------\n",
        "            if use_proj_this_epoch:\n",
        "                idx = 0\n",
        "                for p_param in dp_net.parameters():\n",
        "                    n = p_param.numel()\n",
        "                    p_param.grad = g_update[idx: idx + n].view_as(p_param)\n",
        "                    idx += n\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                outer_step_noDP(dp_net, optimizer, batch_v)\n",
        "            # ----------------------------------------------------------------\n",
        "\n",
        "            # <<< NEW: on‑the‑fly projection diagnostics (always on) --------\n",
        "            with torch.no_grad():\n",
        "                diff = (p - p_prime).norm().item()\n",
        "                cos  = torch.dot(p, p_prime).item() / (\n",
        "                       p.norm().item() * p_prime.norm().item() + 1e-12)\n",
        "                local_diff_L2.append(diff)\n",
        "                local_cos.append(cos)\n",
        "            # ----------------------------------------------------------------\n",
        "\n",
        "            # diagnostics (loss etc.) ---------------------------------------\n",
        "            with torch.no_grad():\n",
        "                batch_loss = F.cross_entropy(dp_net(X), y).item()\n",
        "            losses.append(batch_loss)\n",
        "            recent_losses.append(batch_loss)\n",
        "            if len(recent_losses) > 10:\n",
        "                recent_losses.pop(0)\n",
        "\n",
        "            if batch_idx % diag_every == 0 or batch_idx == total_batches:\n",
        "                secs_per_batch = (time.time() - epoch_start) / batch_idx\n",
        "                tag = \"ProjSGD\" if use_proj_this_epoch else \"SGD\"\n",
        "                fmt = (f\"[{tag}] Epoch {epoch}/{num_epochs}  \"\n",
        "                       f\"Batch {batch_idx:>4}/{total_batches}  \"\n",
        "                       f\"η≈{secs_per_batch:.2f}s  \"\n",
        "                       f\"run-loss={statistics.mean(recent_losses):.4f}  \"\n",
        "                       f\"‖p-p′‖₂={diff:.4f}  cos={cos:.4f}\")\n",
        "                print_memory_usage(fmt)\n",
        "        # -------------------------------------------------------------------\n",
        "        scheduler.step()\n",
        "        epoch_losses.append(np.mean(losses))\n",
        "        epoch_accs.append(evaluate(dp_net, test_loader))\n",
        "\n",
        "        minutes = (time.time() - epoch_start) / 60\n",
        "        tag = \"ProjSGD\" if use_proj_this_epoch else \"SGD\"\n",
        "        print(f\"[{tag}] Epoch {epoch} done  \"\n",
        "              f\"loss={epoch_losses[-1]:.3f}  \"\n",
        "              f\"acc={epoch_accs[-1]:.2f}%  \"\n",
        "              f\"({minutes:.1f} min)\\n\")\n",
        "\n",
        "    # <<< NEW: return extra diagnostics -------------------------------------\n",
        "    return {\"loss\": epoch_losses,\n",
        "            \"acc\": epoch_accs,\n",
        "            \"diff_L2\": local_diff_L2,\n",
        "            \"cos\": local_cos}\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "###############################################################################\n",
        "# 10-B. main_run\n",
        "###############################################################################\n",
        "def main_run():\n",
        "    print_memory_usage(\"Start\")\n",
        "\n",
        "    # A) build virtual subsets\n",
        "    v_subsets = build_virtual_subsets(train_ds,subsets_per_class,subset_size,\n",
        "                                      virtual_augment_factor)\n",
        "    print(f\"Built {len(v_subsets)} virtual subsets.\")\n",
        "\n",
        "    # B) compute e_i for every virtual subset\n",
        "    base_net = ResNet20().to(device).eval()\n",
        "    e_list = [compute_fullmodel_grad_for_subset(base_net,idxs,train_ds)\n",
        "              for idxs,_ in v_subsets]\n",
        "    e_basis = gram_schmidt(e_list)\n",
        "    print(f\"Orthonormal basis size = {len(e_basis)}/{len(e_list)}\")\n",
        "\n",
        "    def project_onto(vec,basis):\n",
        "        out=torch.zeros_like(vec)\n",
        "        for b in basis: out += torch.dot(vec,b)*b\n",
        "        return out\n",
        "\n",
        "\n",
        "    # ProjSGD (after warm‑start)\n",
        "    dp_proj = build_model().to(device)\n",
        "    opt_proj = optim.SGD(dp_proj.parameters(),lr=lr,momentum=outer_momentum,\n",
        "                         weight_decay=5e-4)\n",
        "    proj_stats = run_training(dp_proj,opt_proj,True,\n",
        "                               e_basis,project_onto,\n",
        "                               train_loader,test_loader)\n",
        "\n",
        "    ##################################################################\n",
        "    # Visualisations\n",
        "    ##################################################################\n",
        "    epochs = range(1,num_epochs+1)\n",
        "    steps  = range(1,len(proj_stats['diff_L2'])+1)  # <<< NEW\n",
        "\n",
        "    # 0) SGD‑vs‑ProjSGD curves\n",
        "    plt.figure(figsize=(11,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    #plt.plot(epochs,base_stats['loss'],'-o',label='SGD (p)')\n",
        "    plt.plot(epochs,proj_stats['loss'],'-o',label='ProjSGD (p′)')\n",
        "    plt.title(\"Training Loss per Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    #plt.plot(epochs,base_stats['acc'],'-o',label='SGD (p)')\n",
        "    plt.plot(epochs,proj_stats['acc'],'-o',label='ProjSGD (p′)')\n",
        "    plt.title(\"Test Accuracy per Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.legend()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # <<< NEW: projection diagnostics ---------------------------------------\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(steps,proj_stats['diff_L2'],color='red')\n",
        "    plt.title(\"‖p − p′‖₂ over Steps (Proj run)\")\n",
        "    plt.xlabel(\"Training Step\"); plt.ylabel(\"L2 Norm\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(steps,proj_stats['cos'],color='orange')\n",
        "    plt.title(\"Cosine Similarity(p, p′) (Proj run)\")\n",
        "    plt.xlabel(\"Training Step\"); plt.ylabel(\"Cosine\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # Quick stats\n",
        "    print(\"\\nPopulation‑level stats (Proj run):\")\n",
        "    print(f\"  mean‖p − p′‖₂ = {np.mean(proj_stats['diff_L2']):.4f}\")\n",
        "    print(f\"  mean cos(p,p′) = {np.mean(proj_stats['cos']):.4f}\")\n",
        "\n",
        "    print(\"\\nFinal accuracy:\")\n",
        "    #print(f\"  SGD (p)   : {base_stats['acc'][-1]:.2f}%\")\n",
        "    print(f\"  ProjSGD p′: {proj_stats['acc'][-1]:.2f}%\")\n",
        "\n",
        "###############################################################################\n",
        "if __name__ == \"__main__\":\n",
        "    main_run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "NR7ENvXYvC3n",
        "outputId": "1d026b2a-cfb1-4e09-823b-83bc17883b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEM] Start → 949.3 MB\n",
            "Built 2500 virtual subsets.\n",
            "Orthonormal basis size = 2500/2500\n",
            "[MEM] [SGD] Epoch 1/10  Batch   50/50  η≈2.07s  run-loss=2.3037  ‖p-p′‖₂=0.0002  cos=0.3876 → 1,460.2 MB\n",
            "[SGD] Epoch 1 done  loss=2.405  acc=12.51%  (1.8 min)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 8.88 MiB is free. Process 238795 has 39.54 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 188.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-369622880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mmain_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1-369622880.py\u001b[0m in \u001b[0;36mmain_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m     opt_proj = optim.SGD(dp_proj.parameters(),lr=lr,momentum=outer_momentum,\n\u001b[1;32m    421\u001b[0m                          weight_decay=5e-4)\n\u001b[0;32m--> 422\u001b[0;31m     proj_stats = run_training(dp_proj,opt_proj,True,\n\u001b[0m\u001b[1;32m    423\u001b[0m                                \u001b[0me_basis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject_onto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                                train_loader,test_loader)\n",
            "\u001b[0;32m/tmp/ipython-input-1-369622880.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(dp_net, optimizer, want_projection, e_basis, project, train_loader, test_loader, diag_every)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# (new) after warm-start, rebuild ONCE so the basis matches current weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwarm_start_epochs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwant_projection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0me_basis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_e_basis_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# optional periodic refresh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-369622880.py\u001b[0m in \u001b[0;36mbuild_e_basis_for\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mvirtual_augment_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     )\n\u001b[0;32m--> 257\u001b[0;31m     e_list = [\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mcompute_fullmodel_grad_for_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv_subsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-369622880.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m     e_list = [\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mcompute_fullmodel_grad_for_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv_subsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     ]\n",
            "\u001b[0;32m/tmp/ipython-input-1-369622880.py\u001b[0m in \u001b[0;36mcompute_fullmodel_grad_for_subset\u001b[0;34m(model, subset_idxs, dataset)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mparts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[0;32m--> 648\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/hooks.py\u001b[0m in \u001b[0;36mhook\u001b[0;34m(grad_input, _)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opacus/grad_sample/grad_sample_module.py\u001b[0m in \u001b[0;36mcapture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mgrad_sampler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_compute_per_sample_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mgrad_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_sampler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             create_or_accumulate_grad_sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/opacus/grad_sample/conv.py\u001b[0m in \u001b[0;36mcompute_conv_grad_sample\u001b[0;34m(layer, activations, backprops)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# n=batch_sz; o=num_out_channels; p=(num_in_channels/groups)*kernel_sz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mgrad_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"noq,npq->nop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;31m# rearrange the above tensor and extract diagonals.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         grad_sample = grad_sample.view(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 8.88 MiB is free. Process 238795 has 39.54 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 188.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}